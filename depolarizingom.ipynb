{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T09:07:29.013011Z",
     "start_time": "2024-10-01T09:07:29.010952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import pickle\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from data_utils import mnist_preparation\n",
    "from typing import Optional, Dict, List, Any\n",
    "from torch.utils.data import DataLoader, dataloader\n",
    "from tqdm import tqdm\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from pennylane import numpy as np"
   ],
   "id": "e68124e585ac5b19",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T09:07:31.604286Z",
     "start_time": "2024-10-01T09:07:29.279578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = [0,1,2,3]\n",
    "# Download MNIST and prepare transforms\n",
    "mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "                                transforms.Resize((16, 16)),  # Resize to 16x16\n",
    "                                transforms.ToTensor(),\n",
    "                                #transforms.Lambda(lambda img: add_salt_and_pepper_noise(img, salt_prob=0.1, pepper_prob=0.1)),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))  # Normalize\n",
    "                             ]))\n",
    "#train/vali/test 70/15/15 split, see data_utils.py for further details\n",
    "train_dataloader, validation_dataloader, test_dataloader = mnist_preparation(dataset=mnist, labels = labels, train_test_ratio=0.7,batch_size=64, vali_test_ratio=0.5)\n",
    "\n",
    "print(\"Images in the training set: \", len(train_dataloader.dataset), \"\\n Images in the validation set: \", len(validation_dataloader.dataset), \"\\n Images in the test set: \", len(test_dataloader.dataset))"
   ],
   "id": "d83df32538d9c632",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in the training set:  17327 \n",
      " Images in the validation set:  3713 \n",
      " Images in the test set:  3714\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T10:19:19.951429Z",
     "start_time": "2024-10-01T10:19:19.949023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"/Users/jackvittori/Desktop/4layerokkkk/weight4layer.pickle\", \"rb\") as file: \n",
    "    weights = pickle.load(file)\n",
    "for param in weights.values():\n",
    "    param.requires_grad = False"
   ],
   "id": "292888c7155d7ddb",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T09:08:24.934446Z",
     "start_time": "2024-10-01T09:08:24.931930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open('/Users/jackvittori/Desktop/allenamento26sett/om/trhistory.pickle', 'rb') as file: \n",
    "    tr_history = pickle.load(file)\n",
    "weights = tr_history['weights']"
   ],
   "id": "50aac92ac943e187",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T10:19:25.205347Z",
     "start_time": "2024-10-01T10:19:25.202479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p = 0.001\n",
    "\n",
    "def early_evaluation_utils(state: torch.Tensor = None): \n",
    "    first_pair = [0,1]\n",
    "    if state is not None:\n",
    "        # state vector initialization with input\n",
    "        qml.QubitStateVector(state, wires=range(8))\n",
    "    for i in range(4):\n",
    "        for j in range(8):\n",
    "            qml.RX(weights[f'layer_{i}'][j, 0], wires=j)\n",
    "            qml.RY(weights[f'layer_{i}'][j, 1], wires=j)\n",
    "            qml.RZ(weights[f'layer_{i}'][j, 2], wires=j)\n",
    "        for j in range(8):\n",
    "            qml.CNOT(wires=[j, (j + 1) % 8])\n",
    "            qml.DepolarizingChannel(p=p, wires=(j + 1) % 8)\n",
    "    \n",
    "    return qml.probs(wires = first_pair)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T10:19:25.532588Z",
     "start_time": "2024-10-01T10:19:25.529874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mixed_device = qml.device(\"default.mixed\", wires=[0,1,2,3,4,5,6,7,8,9], shots = 50)\n",
    "early_qnode = qml.QNode(early_evaluation_utils, mixed_device)"
   ],
   "id": "369331353db7c08a",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T10:26:56.467881Z",
     "start_time": "2024-10-01T10:19:25.946339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = []\n",
    "for i, (img, label) in tqdm(enumerate(test_dataloader.dataset)):\n",
    "    \n",
    "    if i == 100:\n",
    "        break        \n",
    "    img = img.type(torch.float64)\n",
    "    img = img / torch.linalg.norm(img).view(-1, 1) \n",
    "    probs = early_qnode(img, shots = 50)\n",
    "    prediction = torch.argmax(probs)\n",
    "    result.append((prediction, label))\n",
    "     \n",
    "def calculate_accuracy(data):\n",
    "    correct = sum([1 for label, prediction in data if label == prediction])\n",
    "    return correct, correct / len(data)\n",
    "\n",
    "test_results = calculate_accuracy(result)\n",
    "\n",
    "print(test_results[0], \"elements have been correctly classified over\", len(test_dataloader.dataset), \"total images with an accuracy of \", test_results[1])"
   ],
   "id": "768c7a99812994ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [07:30,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 elements have been correctly classified over 3714 total images with an accuracy of  0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T09:05:46.167279Z",
     "start_time": "2024-10-01T08:58:18.056498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p = 0.03\n",
    "\n",
    "def early_evaluation_utils(state: torch.Tensor = None): \n",
    "    first_pair = [0,1]\n",
    "    if state is not None:\n",
    "        # state vector initialization with input\n",
    "        qml.QubitStateVector(state, wires=range(8))\n",
    "    for i in range(4):\n",
    "        for j in range(8):\n",
    "            qml.RX(weights[f'layer_{i}'][j, 0], wires=j)\n",
    "            qml.RY(weights[f'layer_{i}'][j, 1], wires=j)\n",
    "            qml.RZ(weights[f'layer_{i}'][j, 2], wires=j)\n",
    "        for j in range(8):\n",
    "            qml.CNOT(wires=[j, (j + 1) % 8])\n",
    "            qml.DepolarizingChannel(p=p, wires=(j + 1) % 8)\n",
    "    \n",
    "    return qml.probs(wires = first_pair)\n",
    "\n",
    "mixed_device = qml.device(\"default.mixed\", wires=[0,1,2,3,4,5,6,7,8,9], shots = 50)\n",
    "early_qnode = qml.QNode(early_evaluation_utils, mixed_device)\n",
    "result = []\n",
    "for i, (img, label) in tqdm(enumerate(test_dataloader.dataset)):\n",
    "    \n",
    "    if i == 100:\n",
    "        break        \n",
    "    img = img.type(torch.float64)\n",
    "    img = img / torch.linalg.norm(img).view(-1, 1) \n",
    "    probs = early_qnode(img, shots = 50)\n",
    "    prediction = torch.argmax(probs)\n",
    "    result.append((prediction, label))\n",
    "     \n",
    "def calculate_accuracy(data):\n",
    "    correct = sum([1 for label, prediction in data if label == prediction])\n",
    "    return correct, correct / len(data)\n",
    "\n",
    "test_results = calculate_accuracy(result)\n",
    "\n",
    "print(test_results[0], \"elements have been correctly classified over\", len(test_dataloader.dataset), \"total images with an accuracy of \", test_results[1])"
   ],
   "id": "5f951cbf2a2aaa13",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [07:28,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 elements have been correctly classified over 3714 total images with an accuracy of  0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4cf8ffebc852b9b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
