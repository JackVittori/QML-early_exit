{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import random\n",
    "import pickle\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from data_utils import mnist_preparation\n",
    "from typing import Optional, Dict, List, Any\n",
    "from torch.utils.data import DataLoader, dataloader\n",
    "from tqdm import tqdm\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from pennylane import numpy as np"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels = [0,1,2,3]\n",
    "# Download MNIST and prepare transforms\n",
    "mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "                                transforms.Resize((16, 16)),  # Resize to 16x16\n",
    "                                transforms.ToTensor(),\n",
    "                                #transforms.Lambda(lambda img: add_salt_and_pepper_noise(img, salt_prob=0.1, pepper_prob=0.1)),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))  # Normalize\n",
    "                             ]))\n",
    "#train/vali/test 70/15/15 split, see data_utils.py for further details\n",
    "train_dataloader, validation_dataloader, test_dataloader = mnist_preparation(dataset=mnist, labels = labels, train_test_ratio=0.7,batch_size=64, vali_test_ratio=0.5)\n",
    "\n",
    "print(\"Images in the training set: \", len(train_dataloader.dataset), \"\\n Images in the validation set: \", len(validation_dataloader.dataset), \"\\n Images in the test set: \", len(test_dataloader.dataset))"
   ],
   "id": "d91ea5830d4d26fb",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"/Users/jackvittori/Desktop/no-noise/norumore-training.pickle\", \"rb\") as file: \n",
    "    training_history = pickle.load(file)\n",
    "loss_history = training_history['loss_history']\n",
    "mcm_accuracy = training_history['mcm_accuracy']\n",
    "fm_accuracy = training_history['fm_accuracy']\n",
    "weights = training_history['model_params']\n",
    "\n",
    "for param in weights.values():\n",
    "    param.requires_grad = False"
   ],
   "id": "713d4ef0cdf2af93",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p = 0.0\n",
    "\n",
    "def early_evaluation_utils(state: torch.Tensor = None): \n",
    "    first_pair = [0,1]\n",
    "    if state is not None:\n",
    "        # state vector initialization with input\n",
    "        qml.QubitStateVector(state, wires=range(8))\n",
    "    for i in range(4):\n",
    "        for j in range(8):\n",
    "            qml.RX(weights[f'layer_{i}'][j, 0], wires=j)\n",
    "            qml.RY(weights[f'layer_{i}'][j, 1], wires=j)\n",
    "            qml.RZ(weights[f'layer_{i}'][j, 2], wires=j)\n",
    "        for j in range(8):\n",
    "            qml.CNOT(wires=[j, (j + 1) % 8])\n",
    "            qml.DepolarizingChannel(p=p, wires=(j + 1) % 8)\n",
    "    \n",
    "    return qml.probs(wires = first_pair)\n",
    "\n",
    "def fully_evaluation_utils(state: torch.Tensor = None):\n",
    "    first_pair = [0,1]\n",
    "    second_pair = [2,3]\n",
    "    measurements = []\n",
    "    if state is not None:\n",
    "        # state vector initialization with input\n",
    "        qml.QubitStateVector(state, wires=range(8))\n",
    "    for i in range(4):\n",
    "        for j in range(8):\n",
    "            qml.RX(weights[f'layer_{i}'][j, 0], wires=j)\n",
    "            qml.RY(weights[f'layer_{i}'][j, 1], wires=j)\n",
    "            qml.RZ(weights[f'layer_{i}'][j, 2], wires=j)\n",
    "        for j in range(8):\n",
    "            qml.CNOT(wires=[j, (j + 1) % 8])\n",
    "            qml.DepolarizingChannel(p=p, wires=(j + 1) % 8)\n",
    "            \n",
    "    for w in first_pair: \n",
    "        measurements.append(qml.measure(wires=w, reset=False, postselect=None))\n",
    "    #m_0 = qml.measure(wires = 0, reset=False, postselect=None)\n",
    "    #m_1 = qml.measure(wires = 1, reset=False, postselect=None)\n",
    "    #print('ok meas 1')\n",
    "    for i in range(4, 8):\n",
    "        for j in range(8):\n",
    "            qml.RX(weights[f'layer_{i}'][j, 0], wires=j)\n",
    "            qml.RY(weights[f'layer_{i}'][j, 1], wires=j)\n",
    "            qml.RZ(weights[f'layer_{i}'][j, 2], wires=j)\n",
    "        for j in range(8):\n",
    "            qml.CNOT(wires=[j, (j + 1) % 8])\n",
    "            qml.DepolarizingChannel(p=p, wires=(j + 1) % 8)\n",
    "    \n",
    "    return qml.probs(op = measurements), qml.probs(wires=[2,3])\n",
    "\n",
    "mixed_device = qml.device(\"default.mixed\", wires=[0,1,2,3,4,5,6,7,8,9])\n",
    "late_qnode = qml.QNode(fully_evaluation_utils, mixed_device)\n",
    "early_qnode = qml.QNode(early_evaluation_utils, mixed_device)"
   ],
   "id": "5112a06c3c801247",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "evaluation_results = []\n",
    "early_results = []\n",
    "count_1 = 0 #counter for early classified images\n",
    "count_2 = 0 #counter for final classified images\n",
    "early_correct = 0 #counter for correctly early classified images \n",
    "final_correct = 0 #counter for correctly final classified images\n",
    "executed_layers = 0\n",
    "\n",
    "for i, (img, target) in tqdm(enumerate(test_dataloader.dataset)):\n",
    "    if i == 100:\n",
    "        break \n",
    "        \n",
    "    img = img.type(torch.float64)\n",
    "    img = img / torch.linalg.norm(img).view(-1, 1)\n",
    "    \n",
    "    #mid circuit evaluation\n",
    "    early_probs = early_qnode(img)\n",
    "    early_prediction = torch.argmax(early_probs)\n",
    "    confidence = early_probs[early_prediction].item()\n",
    "    early_guess = early_prediction == target\n",
    "    early_results.append(early_guess.item())\n",
    "    \n",
    "    if confidence >= 0.31:\n",
    "        #print('early')\n",
    "        evaluation_results.append(early_guess.item())\n",
    "        count_1 += 1\n",
    "        executed_layers += 4\n",
    "        if early_guess: \n",
    "            early_correct += 1\n",
    "            \n",
    "    else: \n",
    "        #print('post')\n",
    "        final_probs = late_qnode(img)\n",
    "        early_full, final_full = final_probs\n",
    "        final_predictions = torch.argmax(final_full)\n",
    "        final_guess = final_predictions == target\n",
    "        evaluation_results.append(final_guess.item())\n",
    "        count_2 += 1\n",
    "        executed_layers += 12\n",
    "        if final_guess: \n",
    "            final_correct += 1\n",
    "    \n",
    "total_accuracy = sum([1 for i in evaluation_results if i == True])/len(evaluation_results)\n",
    "early_total_accuracy = sum([1 for i in early_results if i == True])/len(early_results)\n",
    "early_exited_accuracy = early_correct/count_1 if count_1 > 0 else 0\n",
    "final_exited_accuracy = final_correct/count_2 if count_2 > 0 else 0"
   ],
   "id": "53c4258e38134022",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
