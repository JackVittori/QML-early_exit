{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "evaluation low",
   "id": "8722dd130f3320b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:08:04.776191Z",
     "start_time": "2024-09-25T21:08:04.770908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from data_utils import mnist_preparation, add_salt_and_pepper_noise \n",
    "from evaluationUtils import calculate_mcm_accuracy\n",
    "from tqdm import tqdm\n",
    "import matplotlib as plt\n",
    "from mcmadaptablemodel import MCMQuantumModel, MCMCircuit\n",
    "from pennylane import Device\n",
    "from pennylane.measurements import StateMP\n",
    "from torch.nn import Module, ParameterDict\n",
    "import matplotlib.pyplot as plt\n",
    "from OriginalModel import FullQuantumModel, QuantumCircuit\n",
    "import warnings\n",
    "from typing import Optional, Dict, List, Any\n",
    "from torch.utils.data import DataLoader, dataloader\n",
    "from time import time\n",
    "import math\n",
    "from pennylane.measurements import MidMeasureMP\n",
    "torch.manual_seed(1234)"
   ],
   "id": "initial_id",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:08:05.526768Z",
     "start_time": "2024-09-25T21:08:05.521110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def early_evaluation_utils(params: Dict, state: torch.Tensor = None): \n",
    "    first_pair = [0,1]\n",
    "    measurements = []\n",
    "    if state is not None:\n",
    "        # state vector initialization with input\n",
    "        qml.QubitStateVector(state, wires=range(8))\n",
    "    for i in range(4):\n",
    "        for j in range(8):\n",
    "            qml.RX(params[f'layer_{i}'][j, 0], wires=j)\n",
    "            qml.RY(params[f'layer_{i}'][j, 1], wires=j)\n",
    "            qml.RZ(params[f'layer_{i}'][j, 2], wires=j)\n",
    "        for j in range(8):\n",
    "            qml.CNOT(wires=[j, (j + 1) % 8])\n",
    "    \n",
    "    for w in first_pair: \n",
    "        measurements.append(qml.measure(wires=w)) #measure first pair of qubits\n",
    "    return measurements\n",
    "\n",
    "def fully_evaluation_utils(params: Dict, state: torch.Tensor = None):\n",
    "    first_pair = [0,1]\n",
    "    second_pair = [2,3]\n",
    "    mcasurements = []\n",
    "    if state is not None:\n",
    "        # state vector initialization with input\n",
    "        qml.QubitStateVector(state, wires=range(8))\n",
    "    for i in range(4):\n",
    "        for j in range(8):\n",
    "            qml.RX(params[f'layer_{i}'][j, 0], wires=j)\n",
    "            qml.RY(params[f'layer_{i}'][j, 1], wires=j)\n",
    "            qml.RZ(params[f'layer_{i}'][j, 2], wires=j)\n",
    "        for j in range(8):\n",
    "            qml.CNOT(wires=[j, (j + 1) % 8])\n",
    "            \n",
    "    for w in first_pair: \n",
    "        mcasurements.append(qml.measure(wires=w)) #measure first pair of qubits\n",
    "\n",
    "    for i in range(4, 8):\n",
    "        for j in range(8):\n",
    "            qml.RX(params[f'layer_{i}'][j, 0], wires=j)\n",
    "            qml.RY(params[f'layer_{i}'][j, 1], wires=j)\n",
    "            qml.RZ(params[f'layer_{i}'][j, 2], wires=j)\n",
    "        for j in range(8):\n",
    "            qml.CNOT(wires=[j, (j + 1) % 8])\n",
    "\n",
    "    for w in second_pair:\n",
    "        mcasurements.append(qml.measure(wires=w))\n",
    "\n",
    "    return mcasurements\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=8)\n",
    "@qml.qnode(dev)  \n",
    "def early_evaluation_ansatz(params: Dict, state: torch.Tensor = None):\n",
    "    early_measurement = early_evaluation_utils(params=params, state=state)\n",
    "    return qml.probs(op=early_measurement)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def fully_evaluation_ansatz(params: Dict, state: torch.Tensor = None):\n",
    "    measurements = fully_evaluation_utils(params=params, state=state)\n",
    "    mid_measurement = measurements[:2]\n",
    "    final_measurement = measurements[2:]\n",
    "    return qml.probs(op=mid_measurement), qml.probs(op=final_measurement)"
   ],
   "id": "1256ea6f4aac1eae",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:08:06.596828Z",
     "start_time": "2024-09-25T21:08:06.591770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluation_routine(dataloader: DataLoader, parameters: Dict, threshold: float):\n",
    "    \n",
    "    evaluation_results = []\n",
    "    early_results = []\n",
    "    count_1 = 0 #counter for early classified images\n",
    "    count_2 = 0 #counter for final classified images\n",
    "    early_correct = 0 #counter for correctly early classified images \n",
    "    final_correct = 0 #counter for correctly final classified images\n",
    "    executed_layers = 0\n",
    "    for img, target in dataloader.dataset:\n",
    "        #img normalization\n",
    "        img = img / torch.linalg.norm(img).view(-1, 1)\n",
    "        \n",
    "        #mid circuit evaluation\n",
    "        early_probs = early_evaluation_ansatz(params=parameters, state=img)\n",
    "        early_prediction = torch.argmax(early_probs, dim=1)\n",
    "        confidence = early_probs[0, early_prediction].item()\n",
    "        early_guess = early_prediction == target\n",
    "        early_results.append(early_guess.item())\n",
    "        \n",
    "        if confidence >= threshold:\n",
    "            evaluation_results.append(early_guess.item())\n",
    "            count_1 += 1\n",
    "            executed_layers += 4\n",
    "            if early_guess: \n",
    "                early_correct += 1\n",
    "            \n",
    "        else: \n",
    "            final_probs = fully_evaluation_ansatz(params=parameters, state=img)\n",
    "            early_full, final_full = final_probs\n",
    "            final_predictions = torch.argmax(final_full, dim=1)\n",
    "            final_guess = final_predictions == target\n",
    "            evaluation_results.append(final_guess.item())\n",
    "            count_2 += 1\n",
    "            executed_layers += 12\n",
    "            \n",
    "            if final_guess: \n",
    "                final_correct += 1\n",
    "    total_accuracy = sum([1 for i in evaluation_results if i == True])/len(evaluation_results)\n",
    "    early_total_accuracy = sum([1 for i in early_results if i == True])/len(early_results)\n",
    "    early_exited_accuracy = early_correct/count_1 if count_1 > 0 else 0\n",
    "    final_exited_accuracy = final_correct/count_2 if count_2 > 0 else 0\n",
    "    \n",
    "    return total_accuracy, early_total_accuracy, early_exited_accuracy, count_1, final_exited_accuracy, count_2, executed_layers"
   ],
   "id": "8109155c2ea7f942",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:08:09.959384Z",
     "start_time": "2024-09-25T21:08:09.954776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def explain_evaluation(dataloader: DataLoader, parameters: Dict, threshold: List[float]):\n",
    "    summary_data = {\n",
    "    'Threshold': [],\n",
    "    'Total Accuracy': [],\n",
    "    '# early exited images': [],\n",
    "    'Early exited Accuracy': [],\n",
    "    'Early total accuracy': [],\n",
    "    '# final classified images': [],\n",
    "    'Final classified Accuracy': [],\n",
    "    \"Executed layers\": []}\n",
    "    \n",
    "    for t in tqdm(threshold):\n",
    "        total_accuracy, early_total_accuracy, early_exited_accuracy, count_1, final_exited_accuracy, count_2, executed_layers = evaluation_routine(dataloader, parameters, t)\n",
    "        summary_data['Threshold'].append(t)\n",
    "        summary_data['Total Accuracy'].append(total_accuracy)\n",
    "        summary_data['# early exited images'].append(count_1)\n",
    "        summary_data['Early exited Accuracy'].append(early_exited_accuracy)\n",
    "        summary_data['Early total accuracy'].append(early_total_accuracy)\n",
    "        summary_data['# final classified images'].append(count_2)\n",
    "        summary_data['Final classified Accuracy'].append(final_exited_accuracy)\n",
    "        summary_data['Executed layers'].append(executed_layers)\n",
    "        \n",
    "    df = pd.DataFrame(summary_data)\n",
    "    return summary_data, df"
   ],
   "id": "15c83f4e0ef65c09",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:08:16.537397Z",
     "start_time": "2024-09-25T21:08:13.780551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = [0,1,2,3]\n",
    "# Download MNIST and prepare transforms\n",
    "mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "                                transforms.Resize((16, 16)),  # Resize to 16x16\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambda img: add_salt_and_pepper_noise(img, salt_prob=0.1, pepper_prob=0.1)),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))  # Normalize\n",
    "                             ]))\n",
    "#train/vali/test 70/15/15 split, see data_utils.py for further details\n",
    "train_dataloader, validation_dataloader, test_dataloader = mnist_preparation(dataset=mnist, labels = labels, train_test_ratio=0.7,batch_size=64, vali_test_ratio=0.5)\n",
    "\n",
    "print(\"Images in the training set: \", len(train_dataloader.dataset), \"\\n Images in the validation set: \", len(validation_dataloader.dataset), \"\\n Images in the test set: \", len(test_dataloader.dataset))"
   ],
   "id": "4ba8409f9dcd04d",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:08:17.141701Z",
     "start_time": "2024-09-25T21:08:17.136944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open(\"/Users/jackvittori/Desktop/lownoise/traininghistory.pickle\", \"rb\") as file: \n",
    "    training_history = pickle.load(file)\n",
    "    \n",
    "loss_history = training_history['loss_history']\n",
    "mcm_accuracy = training_history['mcm_accuracy']\n",
    "fm_accuracy = training_history['fm_accuracy']\n",
    "weights = training_history['model_params']"
   ],
   "id": "2f4627922a5e2c9b",
   "execution_count": 34,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:08:21.751343Z",
     "start_time": "2024-09-25T21:08:21.217014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.style.use('ggplot')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "# Plotting the loss on the first subplot\n",
    "ax1.plot(list(range(50)), loss_history, marker='.', linestyle='-', color='b',\n",
    "         label='Loss per Epoch')\n",
    "ax1.set_title('Training Loss Over Epochs', fontsize=16)\n",
    "ax1.set_xlabel('Epochs', fontsize=14)\n",
    "ax1.set_ylabel('Loss', fontsize=14)\n",
    "ax1.set_xticks(list(range(0, 50, 2)))  # Mostra i tick ogni 5 epochs per ridurre la sovrapposizione\n",
    "ax1.set_ylim(2.4, 2.8)  # Fissa la scala dell'asse y tra 2 e 3\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plotting mcm_accuracy and fm_accuracy on the same plot (ax2)\n",
    "ax2.plot(list(range(50)), mcm_accuracy, marker='.', linestyle='--', color='r',\n",
    "         label='Mid circuit accuracy per epoch')\n",
    "ax2.plot(list(range(50)), fm_accuracy, marker='.', linestyle='--', color='g',\n",
    "         label='Final circuit accuracy per epoch')\n",
    "ax2.set_title('Training Accuracy Over Epochs', fontsize=16)\n",
    "ax2.set_xlabel('Epochs', fontsize=14)\n",
    "ax2.set_ylabel('Accuracy', fontsize=14)\n",
    "ax2.set_xticks(list(range(0, 50, 2)))  # Mostra i tick ogni 5 epochs per ridurre la sovrapposizione\n",
    "ax2.set_ylim(0.20, 0.95)  # Fissa la scala dell'asse y tra 0.20 e 0.95\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/jackvittori/Desktop/lownoise/training010.png', dpi=300)\n",
    "plt.show()"
   ],
   "id": "c037a91861e2d373",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:10:05.323253Z",
     "start_time": "2024-09-25T21:08:33.130564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "early_probs_distribution = []\n",
    "final_probs_distribution = []\n",
    "for img, target in tqdm(validation_dataloader.dataset): \n",
    "    img = img / torch.linalg.norm(img).view(-1, 1)\n",
    "    evaluation = fully_evaluation_ansatz(params=weights, state=img)\n",
    "    early_full, final_full = evaluation\n",
    "    \n",
    "    early_prediction = torch.argmax(early_full, dim=1)\n",
    "    early_probs = early_full[0, early_prediction].item()\n",
    "    \n",
    "    final_prediction = torch.argmax(final_full, dim=1)\n",
    "    final_probs = final_full[0, final_prediction].item()\n",
    "    \n",
    "    early_probs_distribution.append(early_probs)\n",
    "    final_probs_distribution.append(final_probs)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Creazione dell'istogramma per il primo array\n",
    "ax.hist(early_probs_distribution, bins=60, density=False, alpha=0.5, color='g', edgecolor='black', label='Mid circuit probability distribution')\n",
    "\n",
    "# Creazione dell'istogramma per il secondo array\n",
    "ax.hist(final_probs_distribution, bins=60, density=False, alpha=0.5, color='r', edgecolor='black', label='final circuit probability distribution')\n",
    "\n",
    "# Titolo e etichette degli assi\n",
    "ax.set_title('Prediction confidence distribution', fontsize=16)\n",
    "ax.set_xlabel('Prediction confidence', fontsize=14)\n",
    "ax.set_ylabel('Occurrencies', fontsize=14)\n",
    "\n",
    "# Aggiunta della legenda per distinguere le distribuzioni\n",
    "ax.legend()\n",
    "\n",
    "# Griglia e layout\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/Users/jackvittori/Desktop/lownoise/probability_distribution01.png', dpi=300)\n",
    "plt.show()"
   ],
   "id": "1d7a317a4951ae69",
   "execution_count": 36,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:59:43.916890Z",
     "start_time": "2024-09-25T21:10:05.324216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "thresholds = [round(x * 0.01 + 0.26, 2) for x in range(26)]\n",
    "summary, table = explain_evaluation(validation_dataloader, weights, thresholds)"
   ],
   "id": "681bd1f703c52cd6",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:59:43.920249Z",
     "start_time": "2024-09-25T21:59:43.917400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ev_data = {\n",
    "    'summary': summary,\n",
    "    'table': table}\n",
    "\n",
    "with open(\"/Users/jackvittori/Desktop/lownoise/evaluation01.pickle\", \"wb\") as file:\n",
    "    pickle.dump(ev_data, file)"
   ],
   "id": "d3e1e35e927fef6c",
   "execution_count": 38,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MID NOISE",
   "id": "c09e1f660f6aed10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:42:58.380028Z",
     "start_time": "2024-09-26T10:42:58.363776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from data_utils import mnist_preparation, add_salt_and_pepper_noise \n",
    "from evaluationUtils import calculate_mcm_accuracy\n",
    "from tqdm import tqdm\n",
    "import matplotlib as plt\n",
    "from mcmadaptablemodel import MCMQuantumModel, MCMCircuit\n",
    "from pennylane import Device\n",
    "from pennylane.measurements import StateMP\n",
    "from torch.nn import Module, ParameterDict\n",
    "import matplotlib.pyplot as plt\n",
    "from OriginalModel import FullQuantumModel, QuantumCircuit\n",
    "import warnings\n",
    "from typing import Optional, Dict, List, Any\n",
    "from torch.utils.data import DataLoader, dataloader\n",
    "from time import time\n",
    "import math\n",
    "from pennylane.measurements import MidMeasureMP\n",
    "torch.manual_seed(1234)"
   ],
   "id": "c14f91474ad315e6",
   "execution_count": 56,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:43:01.872981Z",
     "start_time": "2024-09-26T10:42:58.913901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = [0,1,2,3]\n",
    "# Download MNIST and prepare transforms\n",
    "mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "                                transforms.Resize((16, 16)),  # Resize to 16x16\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambda img: add_salt_and_pepper_noise(img, salt_prob=0.15, pepper_prob=0.15)),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))  # Normalize\n",
    "                             ]))\n",
    "#train/vali/test 70/15/15 split, see data_utils.py for further details\n",
    "train_dataloader, validation_dataloader, test_dataloader = mnist_preparation(dataset=mnist, labels = labels, train_test_ratio=0.7,batch_size=64, vali_test_ratio=0.5)\n",
    "\n",
    "print(\"Images in the training set: \", len(train_dataloader.dataset), \"\\n Images in the validation set: \", len(validation_dataloader.dataset), \"\\n Images in the test set: \", len(test_dataloader.dataset))"
   ],
   "id": "d890b0918e81e2b0",
   "execution_count": 57,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:43:25.704285Z",
     "start_time": "2024-09-26T10:43:25.700637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open(\"/Users/jackvittori/Desktop/midnoise/file_con_pesi_su_cpu.pkl\", \"rb\") as file:\n",
    "    training_history = pickle.load(file)\n",
    "    \n",
    "loss_history = training_history['loss_history']\n",
    "mcm_accuracy = training_history['mcm_accuracy']\n",
    "fm_accuracy = training_history['fm_accuracy']\n",
    "weights = training_history['model_params']"
   ],
   "id": "a5bd6cd53ca85f52",
   "execution_count": 59,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:43:28.847490Z",
     "start_time": "2024-09-26T10:43:28.287040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.style.use('ggplot')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "# Plotting the loss on the first subplot\n",
    "ax1.plot(list(range(50)), loss_history, marker='.', linestyle='-', color='b',\n",
    "         label='Loss per Epoch')\n",
    "ax1.set_title('Training Loss Over Epochs', fontsize=16)\n",
    "ax1.set_xlabel('Epochs', fontsize=14)\n",
    "ax1.set_ylabel('Loss', fontsize=14)\n",
    "ax1.set_xticks(list(range(0, 50, 2)))  # Mostra i tick ogni 5 epochs per ridurre la sovrapposizione\n",
    "ax1.set_ylim(2.4, 2.8)  # Fissa la scala dell'asse y tra 2 e 3\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plotting mcm_accuracy and fm_accuracy on the same plot (ax2)\n",
    "ax2.plot(list(range(50)), mcm_accuracy, marker='.', linestyle='--', color='r',\n",
    "         label='Mid circuit accuracy per epoch')\n",
    "ax2.plot(list(range(50)), fm_accuracy, marker='.', linestyle='--', color='g',\n",
    "         label='Final circuit accuracy per epoch')\n",
    "ax2.set_title('Training Accuracy Over Epochs', fontsize=16)\n",
    "ax2.set_xlabel('Epochs', fontsize=14)\n",
    "ax2.set_ylabel('Accuracy', fontsize=14)\n",
    "ax2.set_xticks(list(range(0, 50, 2)))  # Mostra i tick ogni 5 epochs per ridurre la sovrapposizione\n",
    "ax2.set_ylim(0.20, 0.95)  # Fissa la scala dell'asse y tra 0.20 e 0.95\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/jackvittori/Desktop/midnoise/training015.png', dpi=300)\n",
    "plt.show()"
   ],
   "id": "9d6c3f3b0b232085",
   "execution_count": 60,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:45:04.252036Z",
     "start_time": "2024-09-26T10:43:34.783484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "early_probs_distribution = []\n",
    "final_probs_distribution = []\n",
    "for img, target in tqdm(validation_dataloader.dataset): \n",
    "    img = img / torch.linalg.norm(img).view(-1, 1)\n",
    "    evaluation = fully_evaluation_ansatz(params=weights, state=img)\n",
    "    early_full, final_full = evaluation\n",
    "    \n",
    "    early_prediction = torch.argmax(early_full, dim=1)\n",
    "    early_probs = early_full[0, early_prediction].item()\n",
    "    \n",
    "    final_prediction = torch.argmax(final_full, dim=1)\n",
    "    final_probs = final_full[0, final_prediction].item()\n",
    "    \n",
    "    early_probs_distribution.append(early_probs)\n",
    "    final_probs_distribution.append(final_probs)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Creazione dell'istogramma per il primo array\n",
    "ax.hist(early_probs_distribution, bins=60, density=False, alpha=0.5, color='g', edgecolor='black', label='Mid circuit probability distribution')\n",
    "\n",
    "# Creazione dell'istogramma per il secondo array\n",
    "ax.hist(final_probs_distribution, bins=60, density=False, alpha=0.5, color='r', edgecolor='black', label='final circuit probability distribution')\n",
    "\n",
    "# Titolo e etichette degli assi\n",
    "ax.set_title('Prediction confidence distribution', fontsize=16)\n",
    "ax.set_xlabel('Prediction confidence', fontsize=14)\n",
    "ax.set_ylabel('Occurrencies', fontsize=14)\n",
    "\n",
    "# Aggiunta della legenda per distinguere le distribuzioni\n",
    "ax.legend()\n",
    "\n",
    "# Griglia e layout\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/Users/jackvittori/Desktop/midnoise/probability_distribution015.png', dpi=300)\n",
    "plt.show()"
   ],
   "id": "59b3a6612cdc5896",
   "execution_count": 61,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T11:36:00.235697Z",
     "start_time": "2024-09-26T10:45:04.252884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "thresholds = [round(x * 0.01 + 0.26, 2) for x in range(26)]\n",
    "summary, table = explain_evaluation(validation_dataloader, weights, thresholds)"
   ],
   "id": "3e97b8648ddf52dc",
   "execution_count": 62,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T11:36:00.238236Z",
     "start_time": "2024-09-26T11:36:00.236272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ev_data = {\n",
    "    'summary': summary,\n",
    "    'table': table}\n",
    "\n",
    "with open(\"/Users/jackvittori/Desktop/midnoise/evaluation015.pickle\", \"wb\") as file:\n",
    "    pickle.dump(ev_data, file)"
   ],
   "id": "cde8ec67774c9146",
   "execution_count": 63,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T11:36:16.247178Z",
     "start_time": "2024-09-26T11:36:16.240961Z"
    }
   },
   "cell_type": "code",
   "source": "table",
   "id": "8eb9d89481dae0a1",
   "execution_count": 65,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HIGH NOISE",
   "id": "8f8f194cb7d092b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T09:26:52.564727Z",
     "start_time": "2024-09-26T09:26:52.558111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from data_utils import mnist_preparation, add_salt_and_pepper_noise \n",
    "from evaluationUtils import calculate_mcm_accuracy\n",
    "from tqdm import tqdm\n",
    "import matplotlib as plt\n",
    "from mcmadaptablemodel import MCMQuantumModel, MCMCircuit\n",
    "from pennylane import Device\n",
    "from pennylane.measurements import StateMP\n",
    "from torch.nn import Module, ParameterDict\n",
    "import matplotlib.pyplot as plt\n",
    "from OriginalModel import FullQuantumModel, QuantumCircuit\n",
    "import warnings\n",
    "from typing import Optional, Dict, List, Any\n",
    "from torch.utils.data import DataLoader, dataloader\n",
    "from time import time\n",
    "import math\n",
    "from pennylane.measurements import MidMeasureMP\n",
    "torch.manual_seed(1234)"
   ],
   "id": "3137ba70e8781e9e",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T09:26:56.092726Z",
     "start_time": "2024-09-26T09:26:53.230717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = [0,1,2,3]\n",
    "# Download MNIST and prepare transforms\n",
    "mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "                                transforms.Resize((16, 16)),  # Resize to 16x16\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambda img: add_salt_and_pepper_noise(img, salt_prob=0.2, pepper_prob=0.2)),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))  # Normalize\n",
    "                             ]))\n",
    "#train/vali/test 70/15/15 split, see data_utils.py for further details\n",
    "train_dataloader, validation_dataloader, test_dataloader = mnist_preparation(dataset=mnist, labels = labels, train_test_ratio=0.7,batch_size=64, vali_test_ratio=0.5)\n",
    "\n",
    "print(\"Images in the training set: \", len(train_dataloader.dataset), \"\\n Images in the validation set: \", len(validation_dataloader.dataset), \"\\n Images in the test set: \", len(test_dataloader.dataset))"
   ],
   "id": "c53159545d651b1",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T09:26:56.096275Z",
     "start_time": "2024-09-26T09:26:56.093711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open(\"/Users/jackvittori/Desktop/highnoise/traininghistory.pickle\", \"rb\") as file: \n",
    "    training_history = pickle.load(file)\n",
    "    \n",
    "loss_history = training_history['loss_history']\n",
    "mcm_accuracy = training_history['mcm_accuracy']\n",
    "fm_accuracy = training_history['fm_accuracy']\n",
    "weights = training_history['model_params']"
   ],
   "id": "9eb15f1e6386b330",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T09:26:59.535276Z",
     "start_time": "2024-09-26T09:26:58.885058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.style.use('ggplot')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "# Plotting the loss on the first subplot\n",
    "ax1.plot(list(range(50)), loss_history, marker='.', linestyle='-', color='b',\n",
    "         label='Loss per Epoch')\n",
    "ax1.set_title('Training Loss Over Epochs', fontsize=16)\n",
    "ax1.set_xlabel('Epochs', fontsize=14)\n",
    "ax1.set_ylabel('Loss', fontsize=14)\n",
    "ax1.set_xticks(list(range(0, 50, 2)))  # Mostra i tick ogni 5 epochs per ridurre la sovrapposizione\n",
    "ax1.set_ylim(2.4, 2.8)  # Fissa la scala dell'asse y tra 2 e 3\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plotting mcm_accuracy and fm_accuracy on the same plot (ax2)\n",
    "ax2.plot(list(range(50)), mcm_accuracy, marker='.', linestyle='--', color='r',\n",
    "         label='Mid circuit accuracy per epoch')\n",
    "ax2.plot(list(range(50)), fm_accuracy, marker='.', linestyle='--', color='g',\n",
    "         label='Final circuit accuracy per epoch')\n",
    "ax2.set_title('Training Accuracy Over Epochs', fontsize=16)\n",
    "ax2.set_xlabel('Epochs', fontsize=14)\n",
    "ax2.set_ylabel('Accuracy', fontsize=14)\n",
    "ax2.set_xticks(list(range(0, 50, 2)))  # Mostra i tick ogni 5 epochs per ridurre la sovrapposizione\n",
    "ax2.set_ylim(0.20, 0.95)  # Fissa la scala dell'asse y tra 0.20 e 0.95\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/jackvittori/Desktop/highnoise/training02.png', dpi=300)\n",
    "plt.show()"
   ],
   "id": "d5d101019bb7e28f",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T09:28:28.088463Z",
     "start_time": "2024-09-26T09:26:59.920983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "early_probs_distribution = []\n",
    "final_probs_distribution = []\n",
    "for img, target in tqdm(validation_dataloader.dataset): \n",
    "    img = img / torch.linalg.norm(img).view(-1, 1)\n",
    "    evaluation = fully_evaluation_ansatz(params=weights, state=img)\n",
    "    early_full, final_full = evaluation\n",
    "    \n",
    "    early_prediction = torch.argmax(early_full, dim=1)\n",
    "    early_probs = early_full[0, early_prediction].item()\n",
    "    \n",
    "    final_prediction = torch.argmax(final_full, dim=1)\n",
    "    final_probs = final_full[0, final_prediction].item()\n",
    "    \n",
    "    early_probs_distribution.append(early_probs)\n",
    "    final_probs_distribution.append(final_probs)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Creazione dell'istogramma per il primo array\n",
    "ax.hist(early_probs_distribution, bins=60, density=False, alpha=0.5, color='g', edgecolor='black', label='Mid circuit probability distribution')\n",
    "\n",
    "# Creazione dell'istogramma per il secondo array\n",
    "ax.hist(final_probs_distribution, bins=60, density=False, alpha=0.5, color='r', edgecolor='black', label='final circuit probability distribution')\n",
    "\n",
    "# Titolo e etichette degli assi\n",
    "ax.set_title('Prediction confidence distribution', fontsize=16)\n",
    "ax.set_xlabel('Prediction confidence', fontsize=14)\n",
    "ax.set_ylabel('Occurrencies', fontsize=14)\n",
    "\n",
    "# Aggiunta della legenda per distinguere le distribuzioni\n",
    "ax.legend()\n",
    "\n",
    "# Griglia e layout\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/Users/jackvittori/Desktop/highnoise/probability_distribution02.png', dpi=300)\n",
    "plt.show()"
   ],
   "id": "3ec17e974bf6a9bb",
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:19:47.520625Z",
     "start_time": "2024-09-26T09:28:28.089482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "thresholds = [round(x * 0.01 + 0.26, 2) for x in range(26)]\n",
    "summary, table = explain_evaluation(validation_dataloader, weights, thresholds)"
   ],
   "id": "677a420e227a3a49",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:19:47.523802Z",
     "start_time": "2024-09-26T10:19:47.521233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ev_data = {\n",
    "    'summary': summary,\n",
    "    'table': table}\n",
    "\n",
    "with open(\"/Users/jackvittori/Desktop/highnoise/evaluation02.pickle\", \"wb\") as file:\n",
    "    pickle.dump(ev_data, file)"
   ],
   "id": "dc30dfb5a8d8e0e8",
   "execution_count": 48,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TRAINING LR = 001",
   "id": "1b2924f27ca7050f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LOW",
   "id": "64a80c604f224416"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from data_utils import mnist_preparation, add_salt_and_pepper_noise \n",
    "from evaluationUtils import calculate_mcm_accuracy\n",
    "from tqdm import tqdm\n",
    "import matplotlib as plt\n",
    "from mcmadaptablemodel import MCMQuantumModel, MCMCircuit\n",
    "from pennylane import Device\n",
    "from pennylane.measurements import StateMP\n",
    "from torch.nn import Module, ParameterDict\n",
    "import matplotlib.pyplot as plt\n",
    "from OriginalModel import FullQuantumModel, QuantumCircuit\n",
    "import warnings\n",
    "from typing import Optional, Dict, List, Any\n",
    "from torch.utils.data import DataLoader, dataloader\n",
    "from time import time\n",
    "import math\n",
    "from pennylane.measurements import MidMeasureMP\n",
    "torch.manual_seed(1234)"
   ],
   "id": "2b4c3a7505b89c73",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = MCMQuantumModel(qubits=8, layers=8, early_exits=[3])",
   "id": "a85f784fde5031f2",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = [0,1,2,3]\n",
    "# Download MNIST and prepare transforms\n",
    "mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "                                transforms.Resize((16, 16)),  # Resize to 16x16\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambda img: add_salt_and_pepper_noise(img, salt_prob=0.1, pepper_prob=0.1)),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))  # Normalize\n",
    "                             ]))\n",
    "#train/vali/test 70/15/15 split, see data_utils.py for further details\n",
    "train_dataloader, validation_dataloader, test_dataloader = mnist_preparation(dataset=mnist, labels = labels, train_test_ratio=0.7,batch_size=64, vali_test_ratio=0.5)\n",
    "\n",
    "print(\"Images in the training set: \", len(train_dataloader.dataset), \"\\n Images in the validation set: \", len(validation_dataloader.dataset), \"\\n Images in the test set: \", len(test_dataloader.dataset))"
   ],
   "id": "f2be2bc825fd619b",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mcm_accuracy, fm_accuracy, loss_history = model.fit(dataloader=train_dataloader, learning_rate=0.01, epochs=40, show_plot=True)",
   "id": "36695e3a66c05089",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trained_params = model.params\n",
    "data = {\n",
    "    'model_params': trained_params,\n",
    "    'mcm_accuracy': mcm_accuracy,\n",
    "    'fm_accuracy': fm_accuracy,\n",
    "    'loss_history': loss_history\n",
    "}\n",
    "\n",
    "with open(\"/Users/jackvittori/Desktop/lr001/lowtraining.pickle\", \"wb\") as file:\n",
    "    pickle.dump(data, file)"
   ],
   "id": "16a3688cfbe5272e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MID TRAIN 001",
   "id": "1871bf7d883c1a57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from data_utils import mnist_preparation, add_salt_and_pepper_noise \n",
    "from evaluationUtils import calculate_mcm_accuracy\n",
    "from tqdm import tqdm\n",
    "import matplotlib as plt\n",
    "from mcmadaptablemodel import MCMQuantumModel, MCMCircuit\n",
    "from pennylane import Device\n",
    "from pennylane.measurements import StateMP\n",
    "from torch.nn import Module, ParameterDict\n",
    "import matplotlib.pyplot as plt\n",
    "from OriginalModel import FullQuantumModel, QuantumCircuit\n",
    "import warnings\n",
    "from typing import Optional, Dict, List, Any\n",
    "from torch.utils.data import DataLoader, dataloader\n",
    "from time import time\n",
    "import math\n",
    "from pennylane.measurements import MidMeasureMP\n",
    "torch.manual_seed(1234)"
   ],
   "id": "be982c919284cd6c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = MCMQuantumModel(qubits=8, layers=8, early_exits=[3])",
   "id": "fd22dbcd7f1fb751",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = [0,1,2,3]\n",
    "# Download MNIST and prepare transforms\n",
    "mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "                                transforms.Resize((16, 16)),  # Resize to 16x16\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambda img: add_salt_and_pepper_noise(img, salt_prob=0.15, pepper_prob=0.15)),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))  # Normalize\n",
    "                             ]))\n",
    "#train/vali/test 70/15/15 split, see data_utils.py for further details\n",
    "train_dataloader, validation_dataloader, test_dataloader = mnist_preparation(dataset=mnist, labels = labels, train_test_ratio=0.7,batch_size=64, vali_test_ratio=0.5)\n",
    "\n",
    "print(\"Images in the training set: \", len(train_dataloader.dataset), \"\\n Images in the validation set: \", len(validation_dataloader.dataset), \"\\n Images in the test set: \", len(test_dataloader.dataset))"
   ],
   "id": "afb01f448a1d12c3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mcm_accuracy, fm_accuracy, loss_history = model.fit(dataloader=train_dataloader, learning_rate=0.01, epochs=40, show_plot=True)",
   "id": "2ead3b32369accf",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trained_params = model.params\n",
    "data = {\n",
    "    'model_params': trained_params,\n",
    "    'mcm_accuracy': mcm_accuracy,\n",
    "    'fm_accuracy': fm_accuracy,\n",
    "    'loss_history': loss_history\n",
    "}\n",
    "\n",
    "with open(\"/Users/jackvittori/Desktop/lr001/midtraining.pickle\", \"wb\") as file:\n",
    "    pickle.dump(data, file)"
   ],
   "id": "effea32355e09b98",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HIGH LR 001",
   "id": "1032824b63a40f29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from data_utils import mnist_preparation, add_salt_and_pepper_noise \n",
    "from evaluationUtils import calculate_mcm_accuracy\n",
    "from tqdm import tqdm\n",
    "import matplotlib as plt\n",
    "from mcmadaptablemodel import MCMQuantumModel, MCMCircuit\n",
    "from pennylane import Device\n",
    "from pennylane.measurements import StateMP\n",
    "from torch.nn import Module, ParameterDict\n",
    "import matplotlib.pyplot as plt\n",
    "from OriginalModel import FullQuantumModel, QuantumCircuit\n",
    "import warnings\n",
    "from typing import Optional, Dict, List, Any\n",
    "from torch.utils.data import DataLoader, dataloader\n",
    "from time import time\n",
    "import math\n",
    "from pennylane.measurements import MidMeasureMP\n",
    "torch.manual_seed(1234)"
   ],
   "id": "447502631a69eccf",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "model = MCMQuantumModel(qubits=8, layers=8, early_exits=[3])",
   "id": "9df637ef3dd10149",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
